{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook documents test suites in Checklist. If you are not already familiar with creating tests in Checklist, consider reading the MFT Examples notebook.\n",
    "\n",
    "## Setup\n",
    "First, let's import the libraries and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.expect import Expect\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "from checklist.test_types import MFT\n",
    "from checklist.test_suite import TestSuite\n",
    "from torch.nn import functional as F\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5429810110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize random seed\n",
    "# Remove this code to experiment with random samples\n",
    "random.seed(123)\n",
    "torch.manual_seed(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model loaded'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# Load pretrained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "device = 'cuda'\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\"Model loaded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Test Suite\n",
    "\n",
    "Checklist can run multiple tests in a test suite. Tests can be grouped by capability and results can be explored in a visual table.\n",
    "\n",
    "We will create a test suite called 'Same Token Prediction' with 3 MFTs. Each MFT will test if the token substituted into the prompt template also appears in the generated text.\n",
    "\n",
    "For example, if we prompt the model with \"The **dog** is running in the zoo\" and the model responds with \"The **dog** looks very happy\", then it passes the test because the same animal appears in the model's response.\n",
    "\n",
    "## Creating the MFTs\n",
    "### MFT 1: Same animal appears in response\n",
    "This MFT uses an `{animal}` placeholder in the template. The expectation function checks that the same animal appears in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The dog is running in the zoo',\n",
       " 'The cat is running in the zoo',\n",
       " 'The giraffe is running in the zoo',\n",
       " 'The aardvark is running in the zoo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editor = Editor()\n",
    "animal_prompts = editor.template(\"The {animal} is running in the zoo\", animal=[\"dog\", \"cat\", \"giraffe\", \"aardvark\"], meta=True)\n",
    "animal_prompts.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_same_animal(x, pred, conf, label=None, meta=None):\n",
    "    return meta['animal'] in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_animal_expect_fn = Expect.single(contains_same_animal)\n",
    "same_animal_test = MFT(**animal_prompts, name='Same animal in response', description='The response contains the same animal mentioned in the prompt.', expect=same_animal_expect_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFT 2: Same country appears in response\n",
    "This MFT uses a `{country}` placeholder in the template. The expectation function checks that the same country appears in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MunchWithAdd({'meta': [{'country': 'Algeria'}, {'country': 'Nigeria'}, {'country': 'Trinidad and Tobago'}, {'country': 'Chile'}, {'country': 'Tuvalu'}, {'country': 'Guinea'}, {'country': 'Djibouti'}, {'country': 'Poland'}, {'country': 'Syria'}, {'country': 'Ghana'}], 'data': ['Earlier today, scientists from Algeria discovered  ', 'Earlier today, scientists from Nigeria discovered  ', 'Earlier today, scientists from Trinidad and Tobago discovered  ', 'Earlier today, scientists from Chile discovered  ', 'Earlier today, scientists from Tuvalu discovered  ', 'Earlier today, scientists from Guinea discovered  ', 'Earlier today, scientists from Djibouti discovered  ', 'Earlier today, scientists from Poland discovered  ', 'Earlier today, scientists from Syria discovered  ', 'Earlier today, scientists from Ghana discovered  ']})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_prompts = editor.template(\"Earlier today, scientists from {country} discovered  \", meta=True, nsamples=10)\n",
    "country_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_same_country(x, pred, conf, label=None, meta=None):\n",
    "    return meta['country'] in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_country_expect_fn = Expect.single(contains_same_country)\n",
    "same_country_test = MFT(**country_prompts, name='Same country in response', description='The response contains the same country mentioned in the prompt.', expect=same_country_expect_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFT 3: Same person appears in response\n",
    "This MFT uses a `{first_name}` placeholder in the template. The expectation function checks that the same first name appears in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MunchWithAdd({'meta': [{'first_name': 'Jonathan'}, {'first_name': 'Larry'}, {'first_name': 'Philip'}, {'first_name': 'Mike'}, {'first_name': 'Suzanne'}, {'first_name': 'Roy'}, {'first_name': 'Jason'}, {'first_name': 'Grace'}, {'first_name': 'Kathleen'}, {'first_name': 'Melissa'}], 'data': ['Jonathan is my neighbor.', 'Larry is my neighbor.', 'Philip is my neighbor.', 'Mike is my neighbor.', 'Suzanne is my neighbor.', 'Roy is my neighbor.', 'Jason is my neighbor.', 'Grace is my neighbor.', 'Kathleen is my neighbor.', 'Melissa is my neighbor.']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_prompts = editor.template(\"{first_name} is my neighbor.\", meta=True, nsamples=10)\n",
    "person_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_same_person(x, pred, conf, label=None, meta=None):\n",
    "    return meta['first_name'] in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_person_expect_fn = Expect.single(contains_same_person)\n",
    "same_person_test = MFT(**person_prompts, name='Same person in response', description='The response contains the same person\\'s first name mentioned in the prompt.', expect=same_person_expect_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the tests to the suite\n",
    "The `TestSuite()` constructor creates an empty test suite. Tests can be added one by one using `suite.add(test)`. The optional `capability` parameter can be used to label and group tests that test similar capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.add(same_animal_test, capability=\"Same Token Prediction\")\n",
    "suite.add(same_country_test, capability=\"Same Token Prediction\")\n",
    "suite.add(same_person_test, capability=\"Same Token Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the predictions\n",
    "Now we define the function that Checklist will use to generate predictions from the model. The predictions need to be returned in the form `([predictions], [scores])`, so we will wrap the `generate_sentences()` function with `PredictorWrapper.wrap_predict()` to automatically create a tuple `([predictions], [1, 1, ...])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(prompt: str) -> str:\n",
    "    token_tensor = tokenizer.encode(prompt, return_tensors='pt').to(device) # return_tensors = \"pt\" returns a PyTorch tensor\n",
    "    out = model.generate(\n",
    "        token_tensor,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        num_beams=1,\n",
    "        temperature=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=False,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True)\n",
    "    text = tokenizer.decode(out.sequences[0], skip_special_tokens=True)\n",
    "    return text[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(prompts: List[str]) -> List[str]:\n",
    "    sentences = []\n",
    "    for prompt in prompts:\n",
    "        sentences.append(generate_sentence(prompt))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' Now, let us begin to talk…\\n\\nI heard you guys were coming.\\n: I thought you were from there, right?\\n (Slight sigh, looks back at Vixen) Yeah,',\n",
       "  ' Goodbye, goodbye, dear.\" But when I heard that I felt almost lost.\\n\\nMy wife gave me the same answer of \"Thank you.\"\\n.'],\n",
       " array([1., 1.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_generator = PredictorWrapper.wrap_predict(generate_sentences)\n",
    "wrapped_generator([\"Hello, nice to meet you.\", \"Goodbye, see you later.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the suite\n",
    "We can now run the suite and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Same animal in response\n",
      "Predicting 4 examples\n",
      "Running Same country in response\n",
      "Predicting 10 examples\n",
      "Running Same person in response\n",
      "Predicting 10 examples\n"
     ]
    }
   ],
   "source": [
    "suite.run(wrapped_generator, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(x, pred, conf, label=None, meta=None): \n",
    "    return 'Prompt:      %s\\nCompletion:      %s' % (x, pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Token Prediction\n",
      "\n",
      "Same animal in response\n",
      "Test cases:      4\n",
      "Fails (rate):    4 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      The giraffe is running in the zoo\n",
      "Completion:       when he runs out of time\n",
      "\n",
      "'I was just looking at the wall in 'You're Coming Down' and it said: \"You need to get down and run down to the river, then if\n",
      "----\n",
      "Prompt:      The cat is running in the zoo\n",
      "Completion:      , but I'll only bring back a one year old. There's one guy at the park that is crazy. He was doing some nice things but it's definitely going to end up being one of those stories that\n",
      "----\n",
      "Prompt:      The dog is running in the zoo\n",
      "Completion:      , she says, and a friend of her walks in on them, shouting things like, \"I was scared, can you please sit down\"?\n",
      "\n",
      "He's also apparently had a seizure and an eye scratch,\n",
      "----\n",
      "\n",
      "\n",
      "Same country in response\n",
      "Test cases:      10\n",
      "Fails (rate):    10 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Earlier today, scientists from Djibouti discovered  \n",
      "Completion:       two new subspecies of the African leopard, including the Eurasian leopards that were found previously to be common in southern France and the French national park in France. They also\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Tuvalu discovered  \n",
      "Completion:      ͡° ̂s o ˄̇ ˈskɹ \" in red light.\n",
      "\n",
      "There has been a major controversy over the identification of ik-d\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Guinea discovered  \n",
      "Completion:       that humans who were infected did not evolve, and that    the most likely explanation is that those infected with  puerperifol (which are known to cause inflammation in humans, both humans\n",
      "----\n",
      "\n",
      "\n",
      "Same person in response\n",
      "Test cases:      10\n",
      "Fails (rate):    10 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Jason is my neighbor.\n",
      "Completion:       He's a former soldier, and I've always hated it when I came home from war. Sometimes your whole family just gets so pissed you'll go and ask what's wrong with your neighbor, you kind of know the answer\n",
      "----\n",
      "Prompt:      Melissa is my neighbor.\n",
      "Completion:      \n",
      "\n",
      "Curtis\n",
      " \"Cunt\" Curtis is a co-owner of the house. He is also the president of Cunt Food Corp, a food service provider. And there was a time he would be\n",
      "----\n",
      "Prompt:      Philip is my neighbor.\n",
      "Completion:       The first thing I said was he was a good guy. And it was an understatement. He was good for me. I like to go to his place so I can get to him. Or whenever a meeting is going\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait as we prepare the table data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8935bbe47e4b2e90eb832ef29f74f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'Same animal in respo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.visual_summary_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using files with test suites\n",
    "\n",
    "Some models cannot be run directly on the same machine that is running the Checklist test suite. For instance, a model might need to run in a specially configured lab environment. In this case, Checklist does not have to receive the predictions from the model directly. The predictions can be saved to a file, then the test suite can check the predictions from the file.\n",
    "\n",
    "## Exporting a test suite to a file\n",
    "First, let's create a file that contains all the prompts that we will send to the model.\n",
    "\n",
    "### Accessing test suite data internally\n",
    "Tests are stored in `suite.tests`, which is a dictionary mapping the test name to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same animal in response\n",
      "Same country in response\n",
      "Same person in response\n"
     ]
    }
   ],
   "source": [
    "for key in suite.tests.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the test information by like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The dog is running in the zoo',\n",
       " 'The cat is running in the zoo',\n",
       " 'The giraffe is running in the zoo',\n",
       " 'The aardvark is running in the zoo']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite.tests['Same animal in response'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'animal': 'dog'},\n",
       " {'animal': 'cat'},\n",
       " {'animal': 'giraffe'},\n",
       " {'animal': 'aardvark'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite.tests['Same animal in response'].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to JSON file with to_raw_file()\n",
    "TestSuite's `to_raw_file()` function exports a test suite to a file. The `format_fn` parameter allows us to control how each example in the suite is printed to the file. We can use `format_fn` to print the examples in a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suite_to_json_file(suite, filename):\n",
    "    class Counter:\n",
    "        def __init__(self):\n",
    "            self.count = 0\n",
    "        def get_count(self):\n",
    "            self.count += 1\n",
    "            return self.count\n",
    "    \n",
    "    counter = Counter()\n",
    "    total_tests = 0\n",
    "    for t in suite.tests.values():\n",
    "        total_tests += len(t.data)\n",
    "        \n",
    "    def json_format_fn(x):\n",
    "        example_id = counter.get_count()\n",
    "        json_str = \"\"\n",
    "        if example_id == 1:\n",
    "            json_str = '{\"examples\": ['\n",
    "        json_str += json.dumps({'content': x, 'id': example_id}) + \",\"\n",
    "        if example_id == total_tests:\n",
    "            # remove trailing comma\n",
    "            json_str = json_str[:len(json_str)-1]\n",
    "            json_str += \"]}\"\n",
    "        return json_str\n",
    "    \n",
    "    suite.to_raw_file(filename, format_fn = json_format_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_to_json_file(suite, 'same_token_suite.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"examples\": [{\"content\": \"The dog is running in the zoo\", \"id\": 1},\r\n",
      "{\"content\": \"The cat is running in the zoo\", \"id\": 2},\r\n",
      "{\"content\": \"The giraffe is running in the zoo\", \"id\": 3},\r\n",
      "{\"content\": \"The aardvark is running in the zoo\", \"id\": 4},\r\n",
      "{\"content\": \"Earlier today, scientists from Algeria discovered  \", \"id\": 5},\r\n",
      "{\"content\": \"Earlier today, scientists from Nigeria discovered  \", \"id\": 6},\r\n",
      "{\"content\": \"Earlier today, scientists from Trinidad and Tobago discovered  \", \"id\": 7},\r\n",
      "{\"content\": \"Earlier today, scientists from Chile discovered  \", \"id\": 8},\r\n",
      "{\"content\": \"Earlier today, scientists from Tuvalu discovered  \", \"id\": 9},\r\n",
      "{\"content\": \"Earlier today, scientists from Guinea discovered  \", \"id\": 10},\r\n",
      "{\"content\": \"Earlier today, scientists from Djibouti discovered  \", \"id\": 11},\r\n",
      "{\"content\": \"Earlier today, scientists from Poland discovered  \", \"id\": 12},\r\n",
      "{\"content\": \"Earlier today, scientists from Syria discovered  \", \"id\": 13},\r\n",
      "{\"content\": \"Earlier today, scientists from Ghana discovered  \", \"id\": 14},\r\n",
      "{\"content\": \"Jonathan is my neighbor.\", \"id\": 15},\r\n",
      "{\"content\": \"Larry is my neighbor.\", \"id\": 16},\r\n",
      "{\"content\": \"Philip is my neighbor.\", \"id\": 17},\r\n",
      "{\"content\": \"Mike is my neighbor.\", \"id\": 18},\r\n",
      "{\"content\": \"Suzanne is my neighbor.\", \"id\": 19},\r\n",
      "{\"content\": \"Roy is my neighbor.\", \"id\": 20},\r\n",
      "{\"content\": \"Jason is my neighbor.\", \"id\": 21},\r\n",
      "{\"content\": \"Grace is my neighbor.\", \"id\": 22},\r\n",
      "{\"content\": \"Kathleen is my neighbor.\", \"id\": 23},\r\n",
      "{\"content\": \"Melissa is my neighbor.\", \"id\": 24}]}"
     ]
    }
   ],
   "source": [
    "cat 'same_token_suite.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the test suite JSON\n",
    "The JSON file we created can be imported back into a Python object by using `json.load()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'The dog is running in the zoo', 'id': 1},\n",
       " {'content': 'The cat is running in the zoo', 'id': 2},\n",
       " {'content': 'The giraffe is running in the zoo', 'id': 3}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "f = open('same_token_suite.json', 'r')\n",
    "suite_dict = json.load(f)\n",
    "f.close()\n",
    "suite_dict['examples'][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions from the loaded data\n",
    "Our data has been loaded into a variable named `suite_dict`. Now we can read each example from `suite_dict` and generate the predictions. Each prediction will be written to another file named `same_token_suite_predictions.json`, which will be sent to Checklist to evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('same_token_suite_predictions.json', 'w') as f:\n",
    "    for example in suite_dict['examples']:\n",
    "        prediction = generate_sentence(example['content'])\n",
    "        prediction = prediction.replace('\"', '\\\"')\n",
    "        f.write(json.dumps({'prediction': prediction, 'id': example['id']}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\": \", too, but the vet said she only had a couple hours to live with her.\", \"id\": 1}\r\n",
      "{\"prediction\": \", but I thought this was cute (he's definitely a feline with his eyes!). I want my cat to grow up and be happy; to be successful and healthy.\", \"id\": 2}\r\n",
      "{\"prediction\": \", after being kept in a girder and fed by others. He's having a bad fit when he makes contact with an adult giraffeater.\\n\\nSome girasses were so scared they had to\", \"id\": 3}\r\n",
      "{\"prediction\": \" to avoid being caught up in a battle at Bauberwald.\\n\\nThe zoo was set up yesterday to try to prevent a possible resurgence of a wild animal that is already in conflict with Berlin\", \"id\": 4}\r\n",
      "{\"prediction\": \"\\u00c2 a massive hole in Earth's crust, allowing an extremely high-resolution search for the source of the new finding.\\n\\nAn image from images from the European Space Agency's Gaia satellite reveals a new\", \"id\": 5}\r\n",
      "{\"prediction\": \"\\ue001 \\ue601\\u00a0 the first-ever study of the intercellular boundary between human\\u00a0 \\u00a0and\\u00a0animal \\u00c2 ~~ _______________________\\nIt's well documented that \\u0627\", \"id\": 6}\r\n",
      "{\"prediction\": \"\\u02bb\\u0391\\u03b5\\u03c3\\u03bd\\u03ae\\u03c2 (white patches), meaning \\\"bright spots\\\" in the sky. (Source: T. Tijsberg and F. Schindler\", \"id\": 7}\r\n",
      "{\"prediction\": \" \\u00a0 a new type of black hole in the Sun, a hot, blackhole, but it's got no energy.\\nIt appears to be a small star with no mass, and astronomers can't even\", \"id\": 8}\r\n",
      "{\"prediction\": \"\\u00a0a new form of the substance known as O 2  \\u00a0 in the O-cell, in its form called a\\u00a0 \\ue7be\\u00a0O2\\u00a0in the \\u315e\", \"id\": 9}\r\n",
      "{\"prediction\": \"\\u00a0a new type of life in Borneo, a continent where both species have only been sighted for so long. Scientists believe that a new species of jellyfish, named Bocutis ful\", \"id\": 10}\r\n",
      "{\"prediction\": \"\\u0301, the smallest round of DNA in the human genome.\\n\\nThe discovery, made for the first time this year using the Human Genome Project, adds to a growing body of\", \"id\": 11}\r\n",
      "{\"prediction\": \"_____ to be the first to explain why these animals had turned out to have two genders. The scientists involved were led by Ewald Tewdchen, a biologist at the Institute of Molecular and Organomet\", \"id\": 12}\r\n",
      "{\"prediction\": \"\\ue00aa of the \\ue831 20,000 year diapered \\u0441 \\u0331m \\u0471 (1,600, 831, 578 \\u0f20\\n\\n\\\"\", \"id\": 13}\r\n",
      "{\"prediction\": \"_____________________________________ _______ with their discovery of a new type of chiral, a key element required to prevent the formation of an electron in the ribosome. The discovery will also help in understanding what makes\", \"id\": 14}\r\n",
      "{\"prediction\": \" He likes to bring food to the dogs. The dogs are getting up late at night and the kids are just on the loose. Sometimes the dog has so much food he cannot get enough. I always try to have the children\", \"id\": 15}\r\n",
      "{\"prediction\": \" In fact, I'm even closer with this kid than my fellow neighbours have ever been. He's a real cool guy, the best boy I know was raised by my grandparents and I think that's exactly what he loves to\", \"id\": 16}\r\n",
      "{\"prediction\": \"\\n\\nI used to think he'd call me a coward to avoid talking, but that's not who he is. I'm just a regular guy who likes his kids to be funny. He really likes my life.\", \"id\": 17}\r\n",
      "{\"prediction\": \" I told him I'd do her a favor. He didn't care. She could take a step back and kiss I was sitting on her forehead. For his own protection, he kept his nose down until I did so.\", \"id\": 18}\r\n",
      "{\"prediction\": \" I have nothing to hide and she is a good friend of mine in fact I believe she doesn't know her and I'm really surprised nothing has changed...\\n\\nHi I know all this, but my cat, Lily\", \"id\": 19}\r\n",
      "{\"prediction\": \" He is a sweetheart of mine, and he has already told me that. I will not forget that, though he thinks I am so. One must never underestimate my power and authority.\\n\\nI knew he was not\", \"id\": 20}\r\n",
      "{\"prediction\": \" The man always comes to visit. He is there to see and hear my neighbors, but if you bring him to the house, he will go to get a drink of water when I come back to it. That's what\", \"id\": 21}\r\n",
      "{\"prediction\": \" Thank you very much.\\\"\\n\\nThe news of Clinton's nomination comes in the wake of the Republican attack on her by Breitbart News columnist Donald Trump by questioning his \\\"complete lack of temperament.\\\"\", \"id\": 22}\r\n",
      "{\"prediction\": \" We are about to cross the country together. So I'll have to say something first \\u2013 I'm actually going to get my son an MOSFET, which is a form of breast augmentation, but my\", \"id\": 23}\r\n",
      "{\"prediction\": \" My mom got me here and she gave me an iPad.\\\"\\n\\nThe two girls, ages 14 to 18, said the school was full of \\\"jock\\\" and \\\"cowards.\\\" But one of the girls said\", \"id\": 24}\r\n"
     ]
    }
   ],
   "source": [
    "cat 'same_token_suite_predictions.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading test results from file\n",
    "TestSuite has a `run_from_file()` function that reads the predictions line by line from a file. The `format_fn` parameter is used to parse each line of the file. Our format function, `read_json_prediction()`, converts the JSON object into the predicted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_prediction(x):\n",
    "    test_output = json.load(x)\n",
    "    return test_output['prediction']\n",
    "suite.run_from_file('same_token_suite_predictions.json', file_format='pred_only', format_fn = read_json_prediction, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same Token Prediction\n",
      "\n",
      "Same animal in response\n",
      "Test cases:      4\n",
      "Fails (rate):    2 (50.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      The dog is running in the zoo\n",
      "Completion:      {\"prediction\": \", too, but the vet said she only had a couple hours to live with her.\", \"id\": 1}\n",
      "----\n",
      "Prompt:      The aardvark is running in the zoo\n",
      "Completion:      {\"prediction\": \" to avoid being caught up in a battle at Bauberwald.\\n\\nThe zoo was set up yesterday to try to prevent a possible resurgence of a wild animal that is already in conflict with Berlin\", \"id\": 4}\n",
      "----\n",
      "\n",
      "\n",
      "Same country in response\n",
      "Test cases:      10\n",
      "Fails (rate):    10 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Earlier today, scientists from Ghana discovered  \n",
      "Completion:      {\"prediction\": \"_____________________________________ _______ with their discovery of a new type of chiral, a key element required to prevent the formation of an electron in the ribosome. The discovery will also help in understanding what makes\", \"id\": 14}\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Poland discovered  \n",
      "Completion:      {\"prediction\": \"_____ to be the first to explain why these animals had turned out to have two genders. The scientists involved were led by Ewald Tewdchen, a biologist at the Institute of Molecular and Organomet\", \"id\": 12}\n",
      "----\n",
      "Prompt:      Earlier today, scientists from Djibouti discovered  \n",
      "Completion:      {\"prediction\": \"\\u0301, the smallest round of DNA in the human genome.\\n\\nThe discovery, made for the first time this year using the Human Genome Project, adds to a growing body of\", \"id\": 11}\n",
      "----\n",
      "\n",
      "\n",
      "Same person in response\n",
      "Test cases:      10\n",
      "Fails (rate):    10 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Prompt:      Jonathan is my neighbor.\n",
      "Completion:      {\"prediction\": \" He likes to bring food to the dogs. The dogs are getting up late at night and the kids are just on the loose. Sometimes the dog has so much food he cannot get enough. I always try to have the children\", \"id\": 15}\n",
      "----\n",
      "Prompt:      Roy is my neighbor.\n",
      "Completion:      {\"prediction\": \" He is a sweetheart of mine, and he has already told me that. I will not forget that, though he thinks I am so. One must never underestimate my power and authority.\\n\\nI knew he was not\", \"id\": 20}\n",
      "----\n",
      "Prompt:      Suzanne is my neighbor.\n",
      "Completion:      {\"prediction\": \" I have nothing to hide and she is a good friend of mine in fact I believe she doesn't know her and I'm really surprised nothing has changed...\\n\\nHi I know all this, but my cat, Lily\", \"id\": 19}\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait as we prepare the table data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a459777f164e48839f1e643f6953eb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SuiteSummarizer(stats={'npassed': 0, 'nfailed': 0, 'nfiltered': 0}, test_infos=[{'name': 'Same animal in respo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suite.visual_summary_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
